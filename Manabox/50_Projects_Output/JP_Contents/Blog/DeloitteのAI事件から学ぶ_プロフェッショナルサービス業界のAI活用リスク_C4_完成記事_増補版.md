# 【増補版】DeloitteのAI事件から学ぶプロフェッショナルサービス業界のAI活用リスク

こんにちは！マナラボの菅野です。

AIが私たちの仕事に欠かせないツールとなりつつある今、多くの企業が業務効率化のためにAI導入を進めています。しかしその一方で、AIの思わぬ落とし穴に気づかず、大きな失敗をしてしまうケースも出てきました。

特に、世界最大手のコンサルティング会社DeloitteがAIを使った報告書で大失敗をしてしまったニュースは、私たちに多くの教訓を与えてくれます。

「AIを導入したいけど、何に気をつければいいのかわからない」
「専門的な業務でAIを使うのが少し怖い」

もし、あなたがそう感じているなら、この記事はきっとお役に立てるはずです。AI活用の具体的なリスクとその対策を、一緒に学んでいきましょう。

## DeloitteのAI事件とは？発覚の経緯

事件が起きたのは2025年10月。Deloitte Australiaがオーストラリア政府の雇用・職場関係省（DEWR）から受注した、29万ドル（約4,400万円）の報告書が問題の発端でした。この報告書は、情報技術システムが福祉プログラムのコンプライアンス遵守にどう使われているかを評価するという、非常に専門的で重要な内容でした。

Deloitteは、この報告書作成にAzure OpenAI GPT-4oという最新のAIを活用しました。しかし、完成した報告書には驚くべき欠陥があったのです。

具体的には、AIが存在しない学術論文を引用したり、架空の判決文を作り出したりしていました。このような現象は、AIが事実に基づかない情報をあたかも事実であるかのように生成してしまう「ハルシネーション（幻覚）」と呼ばれています。

簡単に言えば、AIがもっともらしく『嘘』をついてしまった、ということですね。

## 具体的な失敗内容：AIはなぜ「嘘」をついたのか？

この報告書の誤りを最初に発見したのは、シドニー大学の研究者クリス・ラッジ氏でした。彼は自身の専門分野の記述を読んでいて、すぐに違和感を覚えたと言います。

報告書には、以下のような深刻な問題が含まれていました。

- **存在しない学術論文の引用**: AIが架空の研究論文を参考文献として挙げていました。
- **架空の判決文の作成**: 連邦裁判所の判事が述べたとする、実際には存在しない判決文が引用されていました。
- **実際には書かれていない本の存在を主張**: 実在する法学教授が、専門外のテーマで存在しない本を書いたとされていました。
- **20箇所以上の不正確な参考文献**: 全体を通して、参考文献の多くが不正確でした。

ラッジ氏が特に驚いたのは、同僚の教授が書いたとされる本のタイトルでした。それは彼女の専門分野とは全く関係がなく、あり得ない内容だったため、「これはAIのハルシネーションか、世界で最も巧妙に隠された秘密のどちらかだ」と直感したそうです。

この事件は、AIがいかに自然に、そして大胆に「嘘」をつくかという事実を浮き彫りにしました。

## AI活用のリスク：優秀な助手はなぜ間違うのか？

「AIを使いたいけど、こんな失敗は怖い」と感じている方も多いのではないでしょうか。その気持ち、とてもよく分かります。

AIは、膨大なデータを学習して、人間が次にどんな言葉を期待しているかを予測して文章を生成します。ものすごく優秀な予測変換機能のようなものです。しかし、AIは人間のように「知らない」とは言えません。情報が不足している場合でも、学習したデータの中から最も「それらしい」答えを創作してしまうことがあるのです。

AIを「非常に優秀で物知りな新入社員」に例えてみましょう。彼はほとんどの質問に即座に答えてくれますが、知らないことがあると、知っている知識を組み合わせて、もっともらしい答えを自分で作り出してしまう癖があるのです。悪気はないのですが、結果として『嘘』になってしまう。これがハルシネーションの正体です。

### プロフェッショナルサービス業界での深刻なリスク

特に、会計、法律、コンサルティングといったプロフェッショナルサービス業界では、情報の正確性とクライアントからの信頼がビジネスの根幹を成します。AIが生成した誤った情報に気づかずに提供してしまえば、クライアントに損害を与え、信頼を失い、最悪の場合は法的な責任を問われることにもなりかねません。

だからこそ、この「優秀な新入社員」の報告は、必ず経験豊富な上司（つまり人間）がファクトチェック（事実確認）をする必要があるのです。

## 他のBig4企業の対応：業界全体への警鐘

Deloitteの失敗は、競合他社にとっても他人事ではありませんでした。他のBig4企業（EY、KPMG、PwC）は、すぐさま自社のAI利用に関する方針を表明しました。

- **EY**: 「AIシステムの出力を常に評価し、正確性と妥当性を確認してから使用する」という厳格な内部ポリシーを強調しました。
- **KPMG**: 「信頼できるAIポリシー」を掲げ、AIの使用に関する透明性を確保し、専門家によるレビューを義務付けていると述べました。
- **PwC**: 「AIには限界があり、AIへの信頼が極めて重要である」と述べ、慎重なAI活用とガバナンスの必要性を訴えました。

これらの反応は、プロフェッショナルサービス業界全体が、AIの利便性だけでなく、そのリスクを真剣に受け止めていることを示しています。

## AIを安全に活用するためのベストプラクティス

では、どうすればAIを安全に、そして効果的に活用できるのでしょうか？明日から実践できる具体的なベストプラクティスを4つご紹介します。

### 1. 「人間による最終チェック」を業務フローに組み込む
最も重要なのは、AIの生成物を絶対にそのまま使わないことです。「AIによる下書き→人間によるファクトチェックと修正」という流れを、必ず業務フローに組み込みましょう。特に、固有名詞、数値、日付、法的な解釈などは、重点的に確認が必要です。

### 2. AIの「得意・不得意」を理解する
AIは、アイデアの壁打ちや文章の要約、構成案の作成といった「創造的な作業」は得意です。しかし、最新の法令や判例の調査、正確性が求められるデータの引用といった「正確性が命の作業」は苦手な場合があります。AIの限界を理解し、作業内容に応じて人間とAIの役割を分けることが賢明です。

### 3. パイロットプロジェクトから始める
いきなり全社的に、あるいは基幹業務にAIを導入するのはリスクが高いです。まずは特定のチームやプロジェクトで試験的に導入し、AI活用のノウハウや注意点を蓄積していく「パイロットプロジェクト」から始めることをお勧めします。

### 4. 複数人でのダブルチェック体制を構築する
重要な意思決定やクライアントへの提出物については、一人の担当者だけでなく、複数の人間でダブルチェック、トリプルチェックを行う体制を構築しましょう。多様な視点が入ることで、一人では見逃してしまうようなAIの誤りにも気づきやすくなります。

## まとめ：AIは敵か、味方か？

DeloitteのAI事件は、私たちにAIとの向き合い方を改めて考えさせるきっかけとなりました。

AIは、決して魔法の杖ではありません。しかし、その特性を正しく理解し、適切な管理体制のもとで活用すれば、これ以上ないほど強力なビジネスパートナーとなります。

AIを「時々、自信満々に宿題をでっち上げてしまう優秀な学生」だと考えてみてください。彼の能力を最大限に引き出しつつ、間違いを正してあげるのが、私たち人間の役割です。

失敗から学び、AIと上手に付き合っていくこと。それこそが、これからの時代に求められるスキルなのかもしれません。

みなさんも、AIを活用する際は、ぜひ「人間のチェック機能」という最強の安全装置を忘れずに。一緒に、賢くAIと協働する未来を作っていきましょう！

